#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import numpy as np
import xarray as xr
import pygrib
import datetime
import pandas as pd
import concurrent.futures
import time
import logging
from pathlib import Path
from tqdm import tqdm
from scipy.interpolate import RectBivariateSpline
from dask.diagnostics import ProgressBar

# ==============================================================================
# --- åŸºæœ¬è¨­å®š (ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç·¨é›†ã—ã¦å®Ÿè¡Œ) ---
# ==============================================================================
# å‡¦ç†å¯¾è±¡æœŸé–“
START_YEAR = 2018
END_YEAR = 2023

# ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° (ãƒžã‚·ãƒ³ã®CPUã‚³ã‚¢æ•°ã«åˆã‚ã›ã¦èª¿æ•´)gpu01:22, gpu02:44
MAX_WORKERS = 22

# --- ãƒ‘ã‚¹è¨­å®š ---
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå­˜åœ¨ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
SCRIPT_DIR = Path(__file__).parent
# MSM GRIB2ãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„)
MSM_DIR = Path("/mnt/gpu01/MSM/")
# å®Œæˆã—ãŸNetCDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆ
OUTPUT_DIR = SCRIPT_DIR / "output_nc"

# ==============================================================================
# --- ãƒ­ã‚°è¨­å®š ---
# ==============================================================================
def setup_logging():
    """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–ã™ã‚‹"""
    log_dir = SCRIPT_DIR / "logs"
    log_dir.mkdir(exist_ok=True)
    log_filename = log_dir / f"conversion_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # nohupã§å®Ÿè¡Œã—ãŸéš›ã«printã¨åŒæ§˜ã«ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¾ã‚Œã‚‹ã‚ˆã†ã«ã€StreamHandlerã‚‚ä½¿ç”¨ã™ã‚‹
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    # tqdmã¯ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯å‡ºåŠ›ã›ãšã€æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã«ã®ã¿è¡¨ç¤ºã™ã‚‹è¨­å®š
    tqdm.pandas(file=os.sys.stderr)

# ==============================================================================
# --- å®šæ•°å®šç¾© ---
# ==============================================================================
# æŠ½å‡ºå¯¾è±¡ã®å¤‰æ•°ãƒªã‚¹ãƒˆ
MEPS_SURFACE_VARS = {'prmsl': 'Prmsl', '10u': 'U10m', '10v': 'V10m'}
MEPS_SURFACE_LVL_VARS = {('t', 2): 'T2m'}
MEPS_PRESSURE_SPEC = {
    975: ['u', 'v', 't'],
    950: ['u', 'v', 't'],
    925: ['u', 'v', 't', 'r'],
    850: ['u', 'v', 't', 'r'],
    500: ['gh', 't', 'r'],
    300: ['gh', 'u', 'v']
}
PRESSURE_LEVELS = sorted(MEPS_PRESSURE_SPEC.keys())

# --- åº§æ¨™æƒ…å ± ---
# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ ¼å­å®šç¾©
MSM_P_LATS = 47.6 - np.arange(253) * 0.1
MSM_P_LONS = 120.0 + np.arange(241) * 0.125
MSM_S_LATS = 47.6 - np.arange(505) * 0.05
MSM_S_LONS = 120.0 + np.arange(481) * 0.0625

# å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®æ ¼å­å®šç¾© (480x480)
OUTPUT_LATS_SLICE = slice(46.95, 23.0)
OUTPUT_LONS_SLICE = slice(120.0, 149.9375)
OUTPUT_LATS = 46.95 - np.arange(480) * 0.05
OUTPUT_LONS = 120.0 + np.arange(480) * 0.0625

# ==============================================================================
# --- é–¢æ•°å®šç¾© ---
# ==============================================================================
def get_pressure_var_name(short_name, level):
    """æ°—åœ§é¢å¤‰æ•°åã‚’ä½œæˆã™ã‚‹"""
    return f"{short_name.upper()}{level}"

def interpolate_grid_fast(data_values, src_lons, src_lats, target_lons, target_lats):
    """
    RectBivariateSplineã‚’ä½¿ç”¨ã—ã¦é«˜é€Ÿã«æ ¼å­å†…æŒ¿ã‚’è¡Œã†ã€‚
    å…¥åŠ›ç·¯åº¦ã¯åŒ—ã‹ã‚‰å—ã€å‡ºåŠ›ã‚‚ãã‚Œã«åˆã‚ã›ã‚‹ã€‚
    """
    # ç·¯åº¦ã‚’æ˜‡é †ã«ã‚½ãƒ¼ãƒˆï¼ˆå—ã‹ã‚‰åŒ—ã¸ï¼‰
    src_lats_sorted = src_lats[::-1]
    data_values_sorted = data_values[::-1, :]
    
    # è£œé–“é–¢æ•°ã‚’ä½œæˆ
    interp_func = RectBivariateSpline(src_lats_sorted, src_lons, data_values_sorted, kx=1, ky=1, s=0)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ç·¯åº¦ã‚‚æ˜‡é †ã«ã—ã¦è£œé–“ã‚’å®Ÿè¡Œ
    target_lats_sorted = target_lats[::-1]
    interp_values_sorted = interp_func(target_lats_sorted, target_lons, grid=True)
    
    # çµæžœã‚’å…ƒã®é †åºï¼ˆåŒ—ã‹ã‚‰å—ï¼‰ã«æˆ»ã—ã¦è¿”ã™
    return interp_values_sorted[::-1, :].astype(np.float32)

def find_msm_files(base_time, msm_dir):
    """æŒ‡å®šã•ã‚ŒãŸåˆæœŸæ™‚åˆ»ã«å¯¾å¿œã™ã‚‹GRIB2ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’æ¤œç´¢ã™ã‚‹"""
    file_paths = {}
    year_month = base_time.strftime('%Y%m')
    date_str_with_hour = base_time.strftime('%Y%m%d%H')
    file_template = "Z__C_RJTD_{datetime}0000_MSM_GPV_Rjp_{product}_{ft_str}_grib2.bin"
    
    # Lsurf (åœ°ä¸Š) ã¨ L-pall (æ°—åœ§é¢) ãƒ•ã‚¡ã‚¤ãƒ«
    for product_type in ['Lsurf', 'L-pall']:
        for ft in [3, 6]:
            key = f"{product_type}_ft{ft}"
            ft_str = f"FH{ft:02d}"
            path = Path(msm_dir) / year_month / file_template.format(datetime=date_str_with_hour, product=product_type, ft_str=ft_str)
            file_paths[key] = path if path.exists() else None
            
    # Prr (é™æ°´é‡) ãƒ•ã‚¡ã‚¤ãƒ«
    for ft_range in ["00-03", "03-06"]:
        key = f"Prr_ft{ft_range}"
        ft_str = f"FH{ft_range}"
        path = Path(msm_dir) / year_month / file_template.format(datetime=date_str_with_hour, product="Prr", ft_str=ft_str)
        file_paths[key] = path if path.exists() else None
        
    return file_paths

def process_grib_files(file_paths):
    """GRIBãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€å¤‰æ•°åã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾žæ›¸ã¨ã—ã¦è¿”ã™"""
    data_vars = {}
    try:
        # --- èª¬æ˜Žå¤‰æ•° (äºˆå ±æ™‚é–“ ft=3, ft=6) ---
        for ft in [3, 6]:
            # åœ°ä¸Šãƒ‡ãƒ¼ã‚¿ (Lsurf)
            if (lsurf_path := file_paths.get(f"Lsurf_ft{ft}")):
                with pygrib.open(str(lsurf_path)) as grbs:
                    for grb in grbs:
                        if grb.shortName in MEPS_SURFACE_VARS:
                            var_name = MEPS_SURFACE_VARS[grb.shortName]
                            data_vars[f"{var_name}_ft{ft}"] = grb.values.astype(np.float32)
                        elif (grb.shortName, grb.level) in MEPS_SURFACE_LVL_VARS:
                            var_name = MEPS_SURFACE_LVL_VARS[(grb.shortName, grb.level)]
                            data_vars[f"{var_name}_ft{ft}"] = grb.values.astype(np.float32)
            
            # æ°—åœ§é¢ãƒ‡ãƒ¼ã‚¿ (L-pall)
            if (lpall_path := file_paths.get(f"L-pall_ft{ft}")):
                with pygrib.open(str(lpall_path)) as grbs:
                    for grb in grbs:
                        if grb.level in MEPS_PRESSURE_SPEC and grb.shortName in MEPS_PRESSURE_SPEC[grb.level]:
                            var_name = get_pressure_var_name(grb.shortName, grb.level)
                            # åœ°ä¸Šæ ¼å­ã«å†…æŒ¿
                            interp_data = interpolate_grid_fast(grb.values, MSM_P_LONS, MSM_P_LATS, MSM_S_LONS, MSM_S_LATS)
                            data_vars[f"{var_name}_ft{ft}"] = interp_data
        
        # --- é™æ°´é‡ (èª¬æ˜Žå¤‰æ•°) ---
        # 0-3æ™‚é–“ç©ç®—é™æ°´é‡
        if (prr_path := file_paths.get("Prr_ft00-03")):
            with pygrib.open(str(prr_path)) as grbs:
                data_vars['Prec_ft3'] = np.sum([g.values for g in grbs], axis=0).astype(np.float32)
        
        # --- é™æ°´é‡ (èª¬æ˜Žå¤‰æ•° + ç›®çš„å¤‰æ•°) ---
        # 3-6æ™‚é–“ãƒ‡ãƒ¼ã‚¿
        if (prr_path := file_paths.get("Prr_ft03-06")):
            with pygrib.open(str(prr_path)) as grbs:
                hourly_prec = {}
                all_values = []
                
                # ãƒ‡ãƒãƒƒã‚°: ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å‡ºåŠ›
                logging.debug(f"Processing precipitation file: {prr_path}")
                
                # 1ã¤ã®ãƒ«ãƒ¼ãƒ—ã§å…¨ã¦ã®å‡¦ç†ã‚’è¡Œã†
                for i, grb in enumerate(grbs):
                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                    logging.debug(f"Message {i}: forecastTime={grb.forecastTime}, "
                                f"startStep={grb.startStep if hasattr(grb, 'startStep') else 'N/A'}, "
                                f"endStep={grb.endStep if hasattr(grb, 'endStep') else 'N/A'}, "
                                f"stepRange={grb.stepRange if hasattr(grb, 'stepRange') else 'N/A'}")
                    
                    # ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
                    all_values.append(grb.values)
                    
                    # forecastTimeãŒ3,4,5ã®å ´åˆã€ãã‚Œãžã‚Œ4,5,6æ™‚é–“ç›®ã¨ã—ã¦ä¿å­˜
                    if grb.forecastTime == 3:
                        hourly_prec['Prec_Target_ft4'] = grb.values.astype(np.float32)
                    elif grb.forecastTime == 4:
                        hourly_prec['Prec_Target_ft5'] = grb.values.astype(np.float32)
                    elif grb.forecastTime == 5:
                        hourly_prec['Prec_Target_ft6'] = grb.values.astype(np.float32)
                
                # èª¬æ˜Žå¤‰æ•°ã¨ã—ã¦3-6æ™‚é–“ç©ç®—é™æ°´é‡ã‚’ä¿å­˜
                if all_values:
                    data_vars['Prec_4_6h_sum'] = np.sum(all_values, axis=0).astype(np.float32)
                
                data_vars.update(hourly_prec)

    except Exception as e:
        logging.error(f"GRIB processing failed. file_paths: {file_paths}, error: {e}", exc_info=True)
        return {}
        
    return data_vars

def process_single_time_to_dataset(base_time, msm_dir):
    """å˜ä¸€æ™‚åˆ»ã®GRIBç¾¤ã‚’å‡¦ç†ã—ã€ãƒ¡ãƒ¢ãƒªä¸Šã«xarray.Datasetã‚’ä½œæˆã—ã¦è¿”ã™"""
    file_paths = find_msm_files(base_time, msm_dir)
    
    # èª²é¡Œã§è¦æ±‚ã•ã‚Œã‚‹å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸€ã¤ã§ã‚‚æ¬ ã‘ã¦ã„ãŸã‚‰ã‚¹ã‚­ãƒƒãƒ—
    required_keys = ['Lsurf_ft3', 'Lsurf_ft6', 'L-pall_ft3', 'L-pall_ft6', 'Prr_ft00-03', 'Prr_ft03-06']
    if any(file_paths.get(key) is None for key in required_keys):
        logging.warning(f"Skipping {base_time}: Missing one or more required GRIB files.")
        return None
        
    data_vars_raw = process_grib_files(file_paths)
    if not data_vars_raw:
        logging.warning(f"Skipping {base_time}: Data processing returned empty.")
        return None

    try:
        xds_vars = {}
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¤‰æ•°ã‚’æŠ½å‡ºã—ã€é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—ã¦xarray.DataArrayã‚’ä½œæˆ
        for name, data in data_vars_raw.items():
            # ä¸€æ—¦ãƒ•ãƒ«ã‚µã‚¤ã‚ºã®åº§æ¨™ã§DataArrayã‚’ä½œæˆ
            da_full = xr.DataArray(data, dims=['lat', 'lon'], coords={'lat': MSM_S_LATS, 'lon': MSM_S_LONS})
            # ç›®çš„ã®é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—
            cropped_data = da_full.sel(lat=OUTPUT_LATS_SLICE, lon=OUTPUT_LONS_SLICE).values
            # timeæ¬¡å…ƒã‚’è¿½åŠ ã—ã¦æ ¼ç´
            xds_vars[name] = (['time', 'lat', 'lon'], np.expand_dims(cropped_data, axis=0))
        
        # ã“ã®æ™‚åˆ»ã®Datasetã‚’ä½œæˆ
        ds = xr.Dataset(
            data_vars=xds_vars,
            coords={'time': pd.to_datetime([base_time]), 'lat': OUTPUT_LATS, 'lon': OUTPUT_LONS}
        )
        return ds
    except Exception as e:
        logging.error(f"Failed to create xarray.Dataset for {base_time}: {e}", exc_info=True)
        return None

def convert_monthly_data(year, month, msm_dir, output_dir, max_workers):
    """
    æŒ‡å®šã•ã‚ŒãŸå¹´æœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã€æœˆæ¬¡NetCDFãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    ã“ã®é–¢æ•°ã¯ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã›ãšã€ãƒ¡ãƒ¢ãƒªä¸Šã§å‡¦ç†ã‚’å®Œçµã•ã›ã¾ã™ã€‚
    """
    month_start_time = time.time()
    logging.info(f"--- Starting conversion for {year}-{month:02d} ---")
    
    output_file = output_dir / f"{year}{month:02d}.nc"
    if output_file.exists():
        logging.info(f"File {output_file} already exists. Skipping.")
        return

    # --- 1. æœˆå†…ã®å…¨æ™‚åˆ»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å‡¦ç†ã—ã€ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒªã‚¹ãƒˆã«æ ¼ç´ ---
    # ã“ã“ã§ã¯ãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿ã¯ç™ºç”Ÿã—ã¾ã›ã‚“ã€‚
    logging.info("Step 1: Processing all time steps in parallel to create in-memory datasets...")
    t_start_parallel = time.time()
    
    start_date = f"{year}-{month:02d}-01"
    end_date_dt = pd.to_datetime(start_date) + pd.offsets.MonthEnd(0)
    date_range = pd.date_range(start=start_date, end=end_date_dt, freq='D')
    base_times = [d + pd.Timedelta(hours=h) for d in date_range for h in range(0, 24, 3)]
    
    datasets_in_month = []
    desc = f"Processing data for {year}-{month:02d}"
    
    with tqdm(total=len(base_times), desc=desc, file=os.sys.stderr, dynamic_ncols=True) as pbar:
        with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_time = {executor.submit(process_single_time_to_dataset, bt, msm_dir): bt for bt in base_times}
            
            for future in concurrent.futures.as_completed(future_to_time):
                try:
                    result_ds = future.result()
                    if result_ds is not None:
                        datasets_in_month.append(result_ds)
                except Exception as e:
                    logging.error(f"A worker process failed for {future_to_time[future]}: {e}", exc_info=True)
                pbar.update(1)
                
    logging.info(f"Finished parallel processing. Time taken: {time.time() - t_start_parallel:.2f} seconds.")
    
    if not datasets_in_month:
        logging.warning(f"No data was processed for {year}-{month:02d}. Skipping file creation.")
        return

    # --- 2. ãƒ¡ãƒ¢ãƒªä¸Šã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµåˆã—ã€ç‰¹å¾´é‡ã‚’è¿½åŠ  ---
    logging.info(f"Step 2: Concatenating {len(datasets_in_month)} datasets for {year}-{month:02d} in memory...")
    t_start_concat = time.time()
    
    datasets_in_month.sort(key=lambda ds: ds.time.values[0])
    monthly_ds = xr.concat(datasets_in_month, dim='time')
    
    logging.info(f"  - Concatenation completed. Time taken: {time.time() - t_start_concat:.2f} seconds.")

    t_start_features = time.time()
    time_coord = monthly_ds.coords['time']
    monthly_ds['dayofyear_sin'] = np.sin(2 * np.pi * time_coord.dt.dayofyear / 366.0).astype(np.float32)
    monthly_ds['dayofyear_cos'] = np.cos(2 * np.pi * time_coord.dt.dayofyear / 366.0).astype(np.float32)
    monthly_ds['hour_sin']      = np.sin(2 * np.pi * time_coord.dt.hour / 24.0).astype(np.float32)
    monthly_ds['hour_cos']      = np.cos(2 * np.pi * time_coord.dt.hour / 24.0).astype(np.float32)
    logging.info(f"  - Adding time features completed. Time taken: {time.time() - t_start_features:.2f} seconds.")
    
    # --- 3. æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ ---
    logging.info(f"Step 3: Saving the final monthly NetCDF file to {output_file}...")
    t_start_save = time.time()

    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¨­å®šï¼ˆtime=1, lat=å…¨ä½“, lon=å…¨ä½“ï¼‰
    chunk_sizes = {'time': 1, 'lat': len(OUTPUT_LATS), 'lon': len(OUTPUT_LONS)}
    monthly_ds = monthly_ds.chunk(chunk_sizes)

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šï¼ˆåœ§ç¸®ã¨ãƒãƒ£ãƒ³ã‚¯ã®ä¸¡æ–¹ã‚’æŒ‡å®šï¼‰
    encoding = {}
    for var in monthly_ds.data_vars:
        encoding[var] = {
            'zlib': True, 
            'complevel': 5,
            'chunksizes': (1, len(OUTPUT_LATS), len(OUTPUT_LONS)) if monthly_ds[var].ndim == 3 else None
        }

    write_job = monthly_ds.to_netcdf(output_file, encoding=encoding, mode='w', engine='h5netcdf', compute=False)
    
    with ProgressBar():
        write_job.compute()
        
    logging.info(f"  - Saving to NetCDF completed. Time taken: {time.time() - t_start_save:.2f} seconds.")
    logging.info(f"Successfully created {output_file}.")

    # --- 4. ðŸ’¡ ãƒ¡ãƒ¢ãƒªè§£æ”¾ ---
    # ã“ã®æœˆã®å‡¦ç†ã§ä½¿ç”¨ã—ãŸãƒ¡ãƒ¢ãƒªä¸Šã®å·¨å¤§ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ˜Žç¤ºçš„ã«å‰Šé™¤
    del datasets_in_month
    del monthly_ds
    logging.info("  - Cleared memory for the next month's processing.")
    
    logging.info(f"Finished conversion for {year}-{month:02d} in {time.time() - month_start_time:.2f} seconds.")

# ==============================================================================
# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨ ---
# ==============================================================================
def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†ã€‚æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã®GRIB2ãƒ‡ãƒ¼ã‚¿ã‚’æœˆã”ã¨ã«å‡¦ç†ã—ã€
    æœˆæ¬¡NetCDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹ï¼š201801.ncï¼‰ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
    """
    setup_logging()
    total_start_time = time.time()
    logging.info("===== MSM GRIB2 to NetCDF Conversion Process Start =====")
    
    # --- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ ---
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    logging.info(f"Input GRIB2 directory: {MSM_DIR}")
    logging.info(f"Output NetCDF directory: {OUTPUT_DIR}")

    # --- æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®ãƒ«ãƒ¼ãƒ—å‡¦ç† ---
    logging.info(f"--- Starting monthly file conversion for the period: {START_YEAR} to {END_YEAR} ---")
    
    total_months = (END_YEAR - START_YEAR + 1) * 12
    processed_months = 0
    conversion_start_time = time.time()

    # 1ãƒ¶æœˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
    for year in range(START_YEAR, END_YEAR + 1):
        for month in range(1, 13):
            processed_months += 1
            logging.info(f"--- Processing month: {processed_months}/{total_months} ({year}-{month:02d}) ---")
            
            # æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›ã¨ä¿å­˜ã‚’å®Ÿè¡Œ
            convert_monthly_data(year, month, MSM_DIR, OUTPUT_DIR, MAX_WORKERS)
            
            # --- é€²æ—çŠ¶æ³ã®å ±å‘Š ---
            elapsed_time = time.time() - conversion_start_time
            if processed_months > 0:
                avg_time_per_month = elapsed_time / processed_months
                remaining_months = total_months - processed_months
                estimated_time_remaining = avg_time_per_month * remaining_months
                
                logging.info(f"Progress: {processed_months}/{total_months} months complete.")
                logging.info(f"  - Elapsed time: {datetime.timedelta(seconds=int(elapsed_time))}")
                logging.info(f"  - Estimated time remaining: {datetime.timedelta(seconds=int(estimated_time_remaining))}")
            
    total_elapsed = time.time() - total_start_time
    logging.info(f"===== All processes finished. Total execution time: {datetime.timedelta(seconds=int(total_elapsed))} =====")

if __name__ == '__main__':
    main()